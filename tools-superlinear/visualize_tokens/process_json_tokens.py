#!/usr/bin/env python3
"""
Process JSON token data from llama.cpp

This script processes the JSON token data generated by llama.cpp when
the LLAMA_TOKEN_DATA_FILE environment variable is set.

It converts the JSONL (JSON Lines) format into a single JSON array
that can be used for further analysis and visualization.
"""

import json
import argparse
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np

def convert_jsonl_to_array(input_file, output_file):
    """Convert JSONL to a single JSON array"""
    tokens = []
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    # Skip empty lines
                    if line.strip():
                        token = json.loads(line)
                        tokens.append(token)
                except json.JSONDecodeError as e:
                    print(f"Error parsing JSON line: {e}")
                    print(f"Problematic line: {line}")
    except Exception as e:
        print(f"Error reading file: {e}")
        return False
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(tokens, f, indent=2)
    except Exception as e:
        print(f"Error writing file: {e}")
        return False
    
    return len(tokens)

def create_probability_plot(tokens, output_file):
    """Create a plot of token probabilities"""
    if not tokens:
        print("No tokens to plot")
        return False
    
    probabilities = [token.get("selected_probability", 0) for token in tokens]
    indices = range(len(probabilities))
    
    # Create plot
    plt.figure(figsize=(12, 8))
    plt.plot(indices, probabilities, 'b-', marker='o', markersize=4)
    plt.title('Token Selection Probabilities')
    plt.xlabel('Token Index')
    plt.ylabel('Probability')
    plt.grid(True, alpha=0.3)
    
    # Add rolling average
    window_size = min(10, len(probabilities))
    if window_size > 1:
        rolling_avg = np.convolve(probabilities, np.ones(window_size)/window_size, mode='valid')
        plt.plot(range(window_size-1, len(probabilities)), rolling_avg, 'r-', 
                 linewidth=2, label=f'{window_size}-token Moving Average')
        plt.legend()
    
    # Add statistics
    mean_prob = np.mean(probabilities)
    median_prob = np.median(probabilities)
    min_prob = min(probabilities)
    max_prob = max(probabilities)
    std_dev = np.std(probabilities)
    
    stats_text = (
        f"Statistics:\n"
        f"Count: {len(probabilities)}\n"
        f"Mean: {mean_prob:.6f}\n"
        f"Median: {median_prob:.6f}\n"
        f"Min: {min_prob:.6f}\n"
        f"Max: {max_prob:.6f}\n"
        f"Std Dev: {std_dev:.6f}"
    )
    
    plt.figtext(0.15, 0.02, stats_text, fontsize=10,
                bbox=dict(facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(output_file)
    return True

def analyze_token_data(tokens):
    """Analyze token data and print statistics"""
    if not tokens:
        print("No tokens to analyze")
        return
    
    probabilities = [token.get("selected_probability", 0) for token in tokens]
    random_values = [token.get("raw_random", 0) for token in tokens if "raw_random" in token]
    
    print("\nToken Statistics:")
    print(f"  Total tokens: {len(tokens)}")
    print(f"  Mean probability: {np.mean(probabilities):.6f}")
    print(f"  Median probability: {np.median(probabilities):.6f}")
    print(f"  Min probability: {min(probabilities):.6f}")
    print(f"  Max probability: {max(probabilities):.6f}")
    print(f"  Std Dev: {np.std(probabilities):.6f}")
    
    if random_values:
        print("\nRandom Number Statistics:")
        print(f"  Total random values: {len(random_values)}")
        print(f"  Mean value: {np.mean(random_values):.6f}")
        print(f"  Median value: {np.median(random_values):.6f}")
        print(f"  Min value: {min(random_values):.6f}")
        print(f"  Max value: {max(random_values):.6f}")
        print(f"  Std Dev: {np.std(random_values):.6f}")

def main():
    parser = argparse.ArgumentParser(description="Process JSON token data from llama.cpp")
    parser.add_argument("input_file", help="Input JSONL file with token data")
    parser.add_argument("--output", "-o", help="Output JSON file (default: token_data.json)",
                        default="token_data.json")
    parser.add_argument("--plot", "-p", help="Generate probability plot (default: token_probs.png)",
                        default="token_probs.png")
    parser.add_argument("--analyze", "-a", action="store_true", help="Print token statistics")
    
    args = parser.parse_args()
    
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"Error: Input file '{input_path}' does not exist", file=sys.stderr)
        return 1
    
    # Load and convert the token data
    tokens = []
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    token = json.loads(line)
                    tokens.append(token)
    except Exception as e:
        print(f"Error reading input file: {e}", file=sys.stderr)
        return 1
    
    # Write converted data to output file
    try:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(tokens, f, indent=2)
        print(f"Processed {len(tokens)} tokens, saved to {args.output}")
    except Exception as e:
        print(f"Error writing output file: {e}", file=sys.stderr)
        return 1
    
    # Generate probability plot
    if create_probability_plot(tokens, args.plot):
        print(f"Probability plot saved to {args.plot}")
    
    # Print analysis
    if args.analyze:
        analyze_token_data(tokens)
    
    return 0

if __name__ == "__main__":
    sys.exit(main())