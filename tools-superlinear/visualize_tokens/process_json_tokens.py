#!/usr/bin/env python3
"""
Process JSON token data from llama.cpp

This script processes the JSON token data generated by llama.cpp when
the LLAMA_TOKEN_DATA_FILE environment variable is set.

It converts the JSONL (JSON Lines) format into a single JSON array
that can be used for further analysis and visualization.
"""

import json
import argparse
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np

def convert_jsonl_to_array(input_file, output_file):
    """Convert JSONL to a single JSON array"""
    tokens = []
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    # Skip empty lines
                    if line.strip():
                        token = json.loads(line)
                        tokens.append(token)
                except json.JSONDecodeError as e:
                    print(f"Error parsing JSON line: {e}")
                    print(f"Problematic line: {line}")
    except Exception as e:
        print(f"Error reading file: {e}")
        return False
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(tokens, f, indent=2)
    except Exception as e:
        print(f"Error writing file: {e}")
        return False
    
    return len(tokens)

def create_probability_plot(tokens, output_file):
    """Create a plot of token probabilities"""
    if not tokens:
        print("No tokens to plot")
        return False
    
    # Extract probabilities from the new format
    selected_probs = []
    top_probs = []  # For the highest probability candidate
    
    for token_data in tokens:
        if 'selected_probability' in token_data:
            selected_probs.append(token_data['selected_probability'])
            
            # Get the highest probability among candidates
            if 'tokens' in token_data and token_data['tokens']:
                max_prob = max(t['probability'] for t in token_data['tokens'])
                top_probs.append(max_prob)
    
    if not selected_probs:
        print("No probability data found in tokens")
        return False
    
    # Create plot
    plt.figure(figsize=(12, 8))
    
    # Plot both selected and top probabilities
    plt.plot(selected_probs, 'b-', marker='o', markersize=4, alpha=0.7, 
             label='Selected Token Probability')
    if top_probs:
        plt.plot(top_probs, 'r-', marker='o', markersize=4, alpha=0.4,
                label='Highest Candidate Probability')
    
    plt.title('Token Selection Probabilities')
    plt.xlabel('Token Index')
    plt.ylabel('Probability')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # Add rolling average for selected probabilities
    window_size = min(10, len(selected_probs))
    if window_size > 1:
        rolling_avg = np.convolve(selected_probs, np.ones(window_size)/window_size, mode='valid')
        plt.plot(range(window_size-1, len(selected_probs)), rolling_avg, 'g-', 
                 linewidth=2, label=f'{window_size}-token Moving Average')
        plt.legend()
    
    # Add statistics
    mean_prob = np.mean(selected_probs)
    median_prob = np.median(selected_probs)
    min_prob = min(selected_probs)
    max_prob = max(selected_probs)
    std_dev = np.std(selected_probs)
    
    stats_text = (
        f"Selected Token Statistics:\n"
        f"Count: {len(selected_probs)}\n"
        f"Mean: {mean_prob:.6f}\n"
        f"Median: {median_prob:.6f}\n"
        f"Min: {min_prob:.6f}\n"
        f"Max: {max_prob:.6f}\n"
        f"Std Dev: {std_dev:.6f}"
    )
    
    plt.figtext(0.15, 0.02, stats_text, fontsize=10,
                bbox=dict(facecolor='white', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(output_file)
    print(f"Probability plot saved to {output_file}")
    return True

def analyze_token_data(tokens):
    """Analyze token data and print statistics"""
    if not tokens:
        print("No tokens to analyze")
        return
    
    # Extract data from the new format
    selected_probs = []
    random_values = []
    selected_token_ids = []
    candidate_counts = []
    
    for token_data in tokens:
        if 'selected_probability' in token_data:
            selected_probs.append(token_data['selected_probability'])
            
        if 'raw_random' in token_data:
            random_values.append(token_data['raw_random'])
            
        if 'selected_token_id' in token_data:
            selected_token_ids.append(token_data['selected_token_id'])
            
        if 'tokens' in token_data:
            candidate_counts.append(len(token_data['tokens']))
    
    if selected_probs:
        print("\nSelected Token Statistics:")
        print(f"  Total tokens: {len(selected_probs)}")
        print(f"  Mean probability: {np.mean(selected_probs):.6f}")
        print(f"  Median probability: {np.median(selected_probs):.6f}")
        print(f"  Min probability: {min(selected_probs):.6f}")
        print(f"  Max probability: {max(selected_probs):.6f}")
        print(f"  Std Dev: {np.std(selected_probs):.6f}")
    
    if random_values:
        print("\nRandom Number Statistics:")
        print(f"  Total random values: {len(random_values)}")
        print(f"  Mean value: {np.mean(random_values):.6f}")
        print(f"  Median value: {np.median(random_values):.6f}")
        print(f"  Min value: {min(random_values):.6f}")
        print(f"  Max value: {max(random_values):.6f}")
        print(f"  Std Dev: {np.std(random_values):.6f}")
        
    if selected_token_ids:
        print("\nToken ID Statistics:")
        print(f"  Total token IDs: {len(selected_token_ids)}")
        print(f"  Unique token IDs: {len(set(selected_token_ids))}")
        print(f"  Min token ID: {min(selected_token_ids)}")
        print(f"  Max token ID: {max(selected_token_ids)}")
        
    if candidate_counts:
        print("\nCandidate Statistics:")
        print(f"  Average candidates per token: {np.mean(candidate_counts):.1f}")
        print(f"  Min candidates: {min(candidate_counts)}")
        print(f"  Max candidates: {max(candidate_counts)}")

def main():
    parser = argparse.ArgumentParser(description="Process JSON token data from llama.cpp")
    parser.add_argument("input_file", help="Input JSONL file with token data")
    parser.add_argument("--output", "-o", help="Output JSON file (default: token_data.json)",
                        default="token_data.json")
    parser.add_argument("--plot", "-p", help="Generate probability plot (default: token_probs.png)",
                        default="token_probs.png")
    parser.add_argument("--analyze", "-a", action="store_true", help="Print token statistics")
    
    args = parser.parse_args()
    
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"Error: Input file '{input_path}' does not exist", file=sys.stderr)
        return 1
    
    # Load and convert the token data
    tokens = []
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        token = json.loads(line)
                        tokens.append(token)
                    except json.JSONDecodeError as e:
                        print(f"Error parsing JSON line: {e}", file=sys.stderr)
                        print(f"Problematic line: {line}", file=sys.stderr)
                        continue
    except Exception as e:
        print(f"Error reading input file: {e}", file=sys.stderr)
        return 1
    
    # Write converted data to output file
    try:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(tokens, f, indent=2)
        print(f"Processed {len(tokens)} tokens, saved to {args.output}")
    except Exception as e:
        print(f"Error writing output file: {e}", file=sys.stderr)
        return 1
    
    # Generate probability plot
    if not create_probability_plot(tokens, args.plot):
        print("Failed to create probability plot", file=sys.stderr)
        return 1
    
    # Print analysis
    if args.analyze:
        analyze_token_data(tokens)
    
    return 0

if __name__ == "__main__":
    sys.exit(main())